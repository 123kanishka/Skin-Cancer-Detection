{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer image models as new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-16T22:26:00.997814Z",
     "iopub.status.busy": "2024-07-16T22:26:00.997263Z",
     "iopub.status.idle": "2024-07-16T22:26:14.723522Z",
     "shell.execute_reply": "2024-07-16T22:26:14.722309Z",
     "shell.execute_reply.started": "2024-07-16T22:26:00.997788Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-16T22:26:14.726389Z",
     "iopub.status.busy": "2024-07-16T22:26:14.725999Z",
     "iopub.status.idle": "2024-07-16T22:26:24.797172Z",
     "shell.execute_reply": "2024-07-16T22:26:24.795932Z",
     "shell.execute_reply.started": "2024-07-16T22:26:14.726352Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-16T22:26:24.799048Z",
     "iopub.status.busy": "2024-07-16T22:26:24.798704Z",
     "iopub.status.idle": "2024-07-16T22:26:29.194561Z",
     "shell.execute_reply": "2024-07-16T22:26:29.193613Z",
     "shell.execute_reply.started": "2024-07-16T22:26:24.799017Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from tqdm.auto import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image features (training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-16T22:26:29.198418Z",
     "iopub.status.busy": "2024-07-16T22:26:29.197391Z"
    }
   },
   "outputs": [],
   "source": [
    "df_eff = pd.read_csv(\"/kaggle/input/isic-inference-effnetv1b0-for-training-data/train_effnetv1b0.csv\")\n",
    "df_eff = df_eff[[\"target_effnetv1b0\"]]\n",
    "gc.collect()\n",
    "df_eva = pd.read_csv(\"/kaggle/input/isic-inference-eva02-for-training-data/train_eva02.csv\")\n",
    "df_eva = df_eva[[\"target_eva02\"]]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/kaggle/input/isic-2024-challenge/train-metadata.csv\")\n",
    "df_test = pd.read_csv(\"/kaggle/input/isic-2024-challenge/test-metadata.csv\")\n",
    "\n",
    "def feature_engineering(df):\n",
    "    # New features to try...\n",
    "    df[\"lesion_size_ratio\"] = df[\"tbp_lv_minorAxisMM\"] / df[\"clin_size_long_diam_mm\"]\n",
    "    df[\"lesion_shape_index\"] = df[\"tbp_lv_areaMM2\"] / (df[\"tbp_lv_perimeterMM\"] ** 2)\n",
    "    df[\"hue_contrast\"] = (df[\"tbp_lv_H\"] - df[\"tbp_lv_Hext\"]).abs()\n",
    "    df[\"luminance_contrast\"] = (df[\"tbp_lv_L\"] - df[\"tbp_lv_Lext\"]).abs()\n",
    "    df[\"lesion_color_difference\"] = np.sqrt(df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2)\n",
    "    df[\"border_complexity\"] = df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_symm_2axis\"]\n",
    "    df[\"color_uniformity\"] = df[\"tbp_lv_color_std_mean\"] / df[\"tbp_lv_radial_color_std_max\"]\n",
    "    df[\"3d_position_distance\"] = np.sqrt(df[\"tbp_lv_x\"] ** 2 + df[\"tbp_lv_y\"] ** 2 + df[\"tbp_lv_z\"] ** 2) \n",
    "    df[\"perimeter_to_area_ratio\"] = df[\"tbp_lv_perimeterMM\"] / df[\"tbp_lv_areaMM2\"]\n",
    "    df[\"lesion_visibility_score\"] = df[\"tbp_lv_deltaLBnorm\"] + df[\"tbp_lv_norm_color\"]\n",
    "    df[\"combined_anatomical_site\"] = df[\"anatom_site_general\"] + \"_\" + df[\"tbp_lv_location\"]\n",
    "    df[\"symmetry_border_consistency\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_norm_border\"]\n",
    "    df[\"color_consistency\"] = df[\"tbp_lv_stdL\"] / df[\"tbp_lv_Lext\"]\n",
    "    \n",
    "    df[\"size_age_interaction\"] = df[\"clin_size_long_diam_mm\"] * df[\"age_approx\"]\n",
    "    df[\"hue_color_std_interaction\"] = df[\"tbp_lv_H\"] * df[\"tbp_lv_color_std_mean\"]\n",
    "    df[\"lesion_severity_index\"] = (df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_eccentricity\"]) / 3\n",
    "    df[\"shape_complexity_index\"] = df[\"border_complexity\"] + df[\"lesion_shape_index\"]\n",
    "    df[\"color_contrast_index\"] = df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"] + df[\"tbp_lv_deltaLBnorm\"]\n",
    "    df[\"log_lesion_area\"] = np.log(df[\"tbp_lv_areaMM2\"] + 1)\n",
    "    df[\"normalized_lesion_size\"] = df[\"clin_size_long_diam_mm\"] / df[\"age_approx\"]\n",
    "    df[\"mean_hue_difference\"] = (df[\"tbp_lv_H\"] + df[\"tbp_lv_Hext\"]) / 2\n",
    "    df[\"std_dev_contrast\"] = np.sqrt((df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2) / 3)\n",
    "    df[\"color_shape_composite_index\"] = (df[\"tbp_lv_color_std_mean\"] + df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_symm_2axis\"]) / 3\n",
    "    df[\"3d_lesion_orientation\"] = np.arctan2(df_train[\"tbp_lv_y\"], df_train[\"tbp_lv_x\"])\n",
    "    df[\"overall_color_difference\"] = (df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"]) / 3\n",
    "    df[\"symmetry_perimeter_interaction\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_perimeterMM\"]\n",
    "    df[\"comprehensive_lesion_index\"] = (df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_eccentricity\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_symm_2axis\"]) / 4\n",
    "\n",
    "    # Taken from: https://www.kaggle.com/code/dschettler8845/isic-detect-skin-cancer-let-s-learn-together\n",
    "    df[\"color_variance_ratio\"] = df[\"tbp_lv_color_std_mean\"] / df[\"tbp_lv_stdLExt\"]\n",
    "    df[\"border_color_interaction\"] = df[\"tbp_lv_norm_border\"] * df[\"tbp_lv_norm_color\"]\n",
    "    df[\"size_color_contrast_ratio\"] = df[\"clin_size_long_diam_mm\"] / df[\"tbp_lv_deltaLBnorm\"]\n",
    "    df[\"age_normalized_nevi_confidence\"] = df[\"tbp_lv_nevi_confidence\"] / df[\"age_approx\"]\n",
    "    df[\"color_asymmetry_index\"] = df[\"tbp_lv_radial_color_std_max\"] * df[\"tbp_lv_symm_2axis\"]\n",
    "    df[\"3d_volume_approximation\"] = df[\"tbp_lv_areaMM2\"] * np.sqrt(df[\"tbp_lv_x\"]**2 + df[\"tbp_lv_y\"]**2 + df[\"tbp_lv_z\"]**2)\n",
    "    df[\"color_range\"] = (df[\"tbp_lv_L\"] - df[\"tbp_lv_Lext\"]).abs() + (df[\"tbp_lv_A\"] - df[\"tbp_lv_Aext\"]).abs() + (df[\"tbp_lv_B\"] - df[\"tbp_lv_Bext\"]).abs()\n",
    "    df[\"shape_color_consistency\"] = df[\"tbp_lv_eccentricity\"] * df[\"tbp_lv_color_std_mean\"]\n",
    "    df[\"border_length_ratio\"] = df[\"tbp_lv_perimeterMM\"] / (2 * np.pi * np.sqrt(df[\"tbp_lv_areaMM2\"] / np.pi))\n",
    "    df[\"age_size_symmetry_index\"] = df[\"age_approx\"] * df[\"clin_size_long_diam_mm\"] * df[\"tbp_lv_symm_2axis\"]\n",
    "    # Until here.\n",
    "    \n",
    "    new_num_cols = [\n",
    "        \"lesion_size_ratio\", \"lesion_shape_index\", \"hue_contrast\",\n",
    "        \"luminance_contrast\", \"lesion_color_difference\", \"border_complexity\",\n",
    "        \"color_uniformity\", \"3d_position_distance\", \"perimeter_to_area_ratio\",\n",
    "        \"lesion_visibility_score\", \"symmetry_border_consistency\", \"color_consistency\",\n",
    "\n",
    "        \"size_age_interaction\", \"hue_color_std_interaction\", \"lesion_severity_index\", \n",
    "        \"shape_complexity_index\", \"color_contrast_index\", \"log_lesion_area\",\n",
    "        \"normalized_lesion_size\", \"mean_hue_difference\", \"std_dev_contrast\",\n",
    "        \"color_shape_composite_index\", \"3d_lesion_orientation\", \"overall_color_difference\",\n",
    "        \"symmetry_perimeter_interaction\", \"comprehensive_lesion_index\",\n",
    "        \n",
    "        \"color_variance_ratio\", \"border_color_interaction\", \"size_color_contrast_ratio\",\n",
    "        \"age_normalized_nevi_confidence\", \"color_asymmetry_index\", \"3d_volume_approximation\",\n",
    "        \"color_range\", \"shape_color_consistency\", \"border_length_ratio\", \"age_size_symmetry_index\",\n",
    "    ]\n",
    "    new_cat_cols = [\"combined_anatomical_site\"]\n",
    "    return df, new_num_cols, new_cat_cols\n",
    "\n",
    "num_cols = [\n",
    "    'age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', \n",
    "    'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', \n",
    "    'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', \n",
    "    'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB',\n",
    "    'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM',\n",
    "    'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color',\n",
    "    'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL',\n",
    "    'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle',\n",
    "    'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z',\n",
    "]\n",
    "df_train[num_cols] = df_train[num_cols].fillna(df_train[num_cols].median())\n",
    "df_train, new_num_cols, new_cat_cols = feature_engineering(df_train.copy())\n",
    "df_test, _, _ = feature_engineering(df_test.copy())\n",
    "num_cols += new_num_cols\n",
    "# anatom_site_general\n",
    "cat_cols = [\"sex\", \"tbp_tile_type\", \"tbp_lv_location\", \"tbp_lv_location_simple\"] + new_cat_cols\n",
    "train_cols = num_cols + cat_cols\n",
    "\n",
    "df_train[\"target_effnetv1b0\"] = df_eff[\"target_effnetv1b0\"]\n",
    "df_train[\"target_eva02\"] = df_eva[\"target_eva02\"]\n",
    "train_cols += [\"target_effnetv1b0\", \"target_eva02\"]\n",
    "\n",
    "del df_eff, df_eva\n",
    "gc.collect()\n",
    "\n",
    "category_encoder = OrdinalEncoder(\n",
    "    categories='auto',\n",
    "    dtype=int,\n",
    "    handle_unknown='use_encoded_value',\n",
    "    unknown_value=-2,\n",
    "    encoded_missing_value=-1,\n",
    ")\n",
    "\n",
    "X_cat = category_encoder.fit_transform(df_train[cat_cols])\n",
    "for c, cat_col in enumerate(cat_cols):\n",
    "    df_train[cat_col] = X_cat[:, c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "\n",
    "df_train[\"fold\"] = -1\n",
    "for idx, (train_idx, val_idx) in enumerate(gkf.split(df_train, df_train[\"target\"], groups=df_train[\"patient_id\"])):\n",
    "    df_train.loc[val_idx, \"fold\"] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, min_tpr: float=0.80):\n",
    "    v_gt = abs(np.asarray(solution.values)-1)\n",
    "    v_pred = np.array([1.0 - x for x in submission.values])\n",
    "    max_fpr = abs(1-min_tpr)\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "    # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    return partial_auc\n",
    "\n",
    "def custom_lgbm_metric(y_true, y_hat):\n",
    "    # TODO: Refactor with the above.\n",
    "    min_tpr = 0.80\n",
    "    v_gt = abs(y_true-1)\n",
    "    v_pred = np.array([1.0 - x for x in y_hat])\n",
    "    max_fpr = abs(1-min_tpr)\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "    # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    return \"pauc80\", partial_auc, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     param = {\n",
    "#         \"objective\": \"binary\",\n",
    "#         # \"metric\": \"custom\",\n",
    "#         \"verbosity\": -1,\n",
    "#         \"boosting_type\": \"gbdt\",\n",
    "#         \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "#         \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "#         \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "#         \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "#         \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "#         \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "#         \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "#         \"device\": \"gpu\"\n",
    "#     }\n",
    "#     scores = []\n",
    "#     for fold in range(N_SPLITS):\n",
    "#         _df_train = df_train[df_train[\"fold\"] != fold].reset_index(drop=True)\n",
    "#         _df_valid = df_train[df_train[\"fold\"] == fold].reset_index(drop=True)\n",
    "#         dtrain = lgb.Dataset(_df_train[train_cols], label=_df_train[\"target\"])\n",
    "#         gbm = lgb.train(param, dtrain)\n",
    "#         preds = gbm.predict(_df_valid[train_cols])\n",
    "#         score = comp_score(_df_valid[[\"target\"]], pd.DataFrame(preds, columns=[\"prediction\"]), \"\")\n",
    "#         scores.append(score)\n",
    "#     return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=20)\n",
    "\n",
    "# print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "# print(\"Best trial:\")\n",
    "# trial = study.best_trial\n",
    "\n",
    "# print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "# print(\"  Params: \")\n",
    "# for key, value in trial.params.items():\n",
    "#     print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_params = {\n",
    "#     'objective': 'binary',\n",
    "#     # \"random_state\": 42,\n",
    "#     \"n_estimators\": 1500,\n",
    "#     'learning_rate': 0.001,\n",
    "#     'bagging_freq': 1,\n",
    "#     'pos_bagging_fraction': 0.75,\n",
    "#     'neg_bagging_fraction': 0.05,\n",
    "#     'feature_fraction': 0.6,\n",
    "#     'lambda_l1': 0.2,\n",
    "#     'lambda_l2': 0.7,\n",
    "#     'num_leaves': 35,\n",
    "#     \"min_data_in_leaf\": 50,\n",
    "#     \"verbosity\": -1,\n",
    "#     \"device\": \"gpu\"\n",
    "#     # \"extra_trees\": True\n",
    "# }\n",
    "new_params =  {\n",
    "    \"objective\": \"binary\",\n",
    "    \"verbosity\": -1,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"n_estimators\": 200,\n",
    "    'learning_rate': 0.05,    \n",
    "    'lambda_l1': 0.0004681884533249742, \n",
    "    'lambda_l2': 8.765240856362274, \n",
    "    'num_leaves': 136, \n",
    "    'feature_fraction': 0.5392005444882538, \n",
    "    'bagging_fraction': 0.9577412548866563, \n",
    "    'bagging_freq': 6,\n",
    "    'min_child_samples': 60,\n",
    "    \"device\": \"gpu\"\n",
    "}\n",
    "lgb_scores = []\n",
    "lgb_models = []\n",
    "for fold in tqdm(range(N_SPLITS)):\n",
    "    _df_train = df_train[df_train[\"fold\"] != fold].reset_index(drop=True)\n",
    "    _df_valid = df_train[df_train[\"fold\"] == fold].reset_index(drop=True)\n",
    "    # model = lgb.LGBMClassifier(**new_params)\n",
    "    model = VotingClassifier([(f\"lgb_{i}\", lgb.LGBMClassifier(random_state=i, **new_params)) for i in range(3)], voting=\"soft\")\n",
    "    model.fit(_df_train[train_cols], _df_train[\"target\"])\n",
    "    preds = model.predict_proba(_df_valid[train_cols])[:, 1]\n",
    "    score = comp_score(_df_valid[[\"target\"]], pd.DataFrame(preds, columns=[\"prediction\"]), \"\")\n",
    "    print(f\"fold: {fold} - Partial AUC Score: {score:.5f}\")\n",
    "    lgb_scores.append(score)\n",
    "    lgb_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_score = np.mean(lgb_scores)\n",
    "print(f\"LGBM Score: {lgbm_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = np.mean([estimator.feature_importances_ for model in lgb_models for estimator in model.estimators_ ], 0)\n",
    "df_imp = pd.DataFrame({\"feature\": lgb_models[0].estimators_[0].feature_name_, \"importance\": importances}).sort_values(\"importance\").reset_index(drop=True)\n",
    "df_imp.to_csv(\"imp_lgbm.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.barh(df_imp[\"feature\"], df_imp[\"importance\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     param = {\n",
    "#         \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
    "#         \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
    "#         \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "#         \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "#         \"bootstrap_type\": trial.suggest_categorical(\n",
    "#             \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
    "#         ),\n",
    "#         # \"task_type\": \"GPU\",\n",
    "#         # \"used_ram_limit\": \"3gb\",\n",
    "#     }\n",
    "#     if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "#         param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "#     elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "#         param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
    "\n",
    "#     scores = []\n",
    "#     for fold in range(N_SPLITS):\n",
    "#         _df_train = df_train[df_train[\"fold\"] != fold].reset_index(drop=True)\n",
    "#         _df_valid = df_train[df_train[\"fold\"] == fold].reset_index(drop=True)\n",
    "#         gbm = cb.CatBoostClassifier(**param)\n",
    "#         gbm.fit(_df_train[train_cols], _df_train[\"target\"], eval_set=[(_df_valid[train_cols], _df_valid[\"target\"])], verbose=0, early_stopping_rounds=100)\n",
    "#         preds = gbm.predict(_df_valid[train_cols])\n",
    "#         score = comp_score(_df_valid[[\"target\"]], pd.DataFrame(preds, columns=[\"prediction\"]), \"\")\n",
    "#         scores.append(score)\n",
    "#     return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=20, timeout=600)\n",
    "\n",
    "# print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "# print(\"Best trial:\")\n",
    "# trial = study.best_trial\n",
    "\n",
    "# print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "# print(\"  Params: \")\n",
    "# for key, value in trial.params.items():\n",
    "#     print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_params = {\n",
    "    'objective': 'Logloss',\n",
    "    # \"random_state\": 42,\n",
    "    \"colsample_bylevel\": 0.3, # 0.01, 0.1\n",
    "    \"iterations\": 400,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"cat_features\": cat_cols,\n",
    "    \"max_depth\": 8,\n",
    "    \"l2_leaf_reg\": 5,\n",
    "    # \"task_type\": \"GPU\",\n",
    "    # \"scale_pos_weight\": 2,\n",
    "    \"verbose\": 0,\n",
    "}\n",
    "cb_scores = []\n",
    "cb_models = []\n",
    "for fold in tqdm(range(N_SPLITS)):\n",
    "    _df_train = df_train[df_train[\"fold\"] != fold].reset_index(drop=True)\n",
    "    _df_valid = df_train[df_train[\"fold\"] == fold].reset_index(drop=True)\n",
    "    model = cb.CatBoostClassifier(**cb_params)\n",
    "    model = VotingClassifier([(f\"cb_{i}\", cb.CatBoostClassifier(random_state=i, **cb_params)) for i in range(3)], voting=\"soft\")\n",
    "    # eval_set=(_df_valid[train_cols], _df_valid[\"target\"]), early_stopping_rounds=50\n",
    "    model.fit(_df_train[train_cols], _df_train[\"target\"])\n",
    "    preds = model.predict_proba(_df_valid[train_cols])[:, 1]\n",
    "    score = comp_score(_df_valid[[\"target\"]], pd.DataFrame(preds, columns=[\"prediction\"]), \"\")\n",
    "    print(f\"fold: {fold} - Partial AUC Score: {score:.5f}\")\n",
    "    cb_scores.append(score)\n",
    "    cb_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_score = np.mean(cb_scores)\n",
    "print(f\"CatBoost Score: {cb_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = np.mean([estimator.feature_importances_ for model in cb_models for estimator in model.estimators_ ], 0)\n",
    "df_imp = pd.DataFrame({\"feature\": cb_models[0].estimators_[0].feature_names_, \"importance\": importances}).sort_values(\"importance\").reset_index(drop=True)\n",
    "df_imp.to_csv(\"imp_catb.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.barh(df_imp[\"feature\"], df_imp[\"importance\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image features (test data)\n",
    "df_eff = pd.read_csv(\"submission_effnetv1b0.csv\")\n",
    "df_test[\"target_effnetv1b0\"] = df_eff[\"target\"]\n",
    "del df_eff\n",
    "gc.collect()\n",
    "\n",
    "df_eva = pd.read_csv(\"submission_eva02.csv\")\n",
    "df_test[\"target_eva02\"] = df_eva[\"target\"]\n",
    "del df_eva\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = category_encoder.transform(df_test[cat_cols])\n",
    "for c, cat_col in enumerate(cat_cols):\n",
    "    df_test[cat_col] = X_cat[:, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "lgb_preds = np.mean([model.predict_proba(df_test[train_cols])[:, 1] for model in lgb_models], 0)\n",
    "cb_preds = np.mean([model.predict_proba(df_test[train_cols])[:, 1] for model in cb_models], 0)\n",
    "preds = lgb_preds * 0.5 + cb_preds * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9094797,
     "sourceId": 63056,
     "sourceType": "competition"
    },
    {
     "sourceId": 186147615,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 187730674,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 188543089,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 188543756,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 188602899,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 188603204,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
